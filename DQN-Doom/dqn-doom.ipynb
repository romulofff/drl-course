{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to play Doom with Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from skimage import transform\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.set_window_visible(False)\n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.set_window_visible(True)\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "game, possible_actions = create_environment()\n",
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "#     print(frame.shape)\n",
    "    x = np.mean(frame, 0)\n",
    "#     print(x.shape)\n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = x[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, (84,84))\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "#     frame = preprocess_frame(state)\n",
    "    frame = state\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-e11219a61091>:29: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From c:\\users\\romul\\documents\\code\\drl-freecodecamp\\venv\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-e11219a61091>:34: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-10-e11219a61091>:86: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-e11219a61091>:94: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\romul\\documents\\code\\drl-freecodecamp\\venv\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state = preprocess_frame(state)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state = preprocess_frame(state)\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        \n",
    "        next_state = preprocess_frame(next_state)\n",
    "        \n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This function will do the part\n",
    "With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 65.0 Training loss: 162.7269 Explore P: 0.9969\n",
      "Model Saved\n",
      "Episode: 1 Total reward: 92.0 Training loss: 284.4552 Explore P: 0.9960\n",
      "Episode: 3 Total reward: 18.0 Training loss: 25.3824 Explore P: 0.9796\n",
      "Episode: 4 Total reward: 91.0 Training loss: 86.6092 Explore P: 0.9787\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 94.0 Training loss: 14.0632 Explore P: 0.9683\n",
      "Episode: 9 Total reward: -3.0 Training loss: 135.1864 Explore P: 0.9415\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 92.0 Training loss: 2.9904 Explore P: 0.9314\n",
      "Episode: 12 Total reward: 95.0 Training loss: 9.0716 Explore P: 0.9309\n",
      "Episode: 13 Total reward: 95.0 Training loss: 4.4595 Explore P: 0.9303\n",
      "Episode: 14 Total reward: 95.0 Training loss: 5.8040 Explore P: 0.9298\n",
      "Episode: 15 Total reward: 93.0 Training loss: 19.6964 Explore P: 0.9290\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 95.0 Training loss: 9.9849 Explore P: 0.9285\n",
      "Episode: 17 Total reward: 7.0 Training loss: 9.4942 Explore P: 0.9217\n",
      "Episode: 19 Total reward: 65.0 Training loss: 13.0178 Explore P: 0.9098\n",
      "Episode: 20 Total reward: 94.0 Training loss: 34.0601 Explore P: 0.9092\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 33.0 Training loss: 29.3185 Explore P: 0.9044\n",
      "Episode: 22 Total reward: -6.0 Training loss: 12.4585 Explore P: 0.8971\n",
      "Episode: 23 Total reward: 95.0 Training loss: 20.0044 Explore P: 0.8966\n",
      "Episode: 24 Total reward: 95.0 Training loss: 18.3077 Explore P: 0.8961\n",
      "Model Saved\n",
      "Episode: 28 Total reward: 94.0 Training loss: 7.1467 Explore P: 0.8693\n",
      "Episode: 30 Total reward: 90.0 Training loss: 5.3146 Explore P: 0.8598\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 76.0 Training loss: 5.4502 Explore P: 0.8581\n",
      "Episode: 32 Total reward: 95.0 Training loss: 11.5138 Explore P: 0.8576\n",
      "Episode: 33 Total reward: 94.0 Training loss: 9.9833 Explore P: 0.8570\n",
      "Episode: 34 Total reward: 94.0 Training loss: 7.7008 Explore P: 0.8564\n",
      "Model Saved\n",
      "Episode: 37 Total reward: 95.0 Training loss: 8.8439 Explore P: 0.8392\n",
      "Episode: 38 Total reward: -11.0 Training loss: 10.7207 Explore P: 0.8316\n",
      "Episode: 39 Total reward: 76.0 Training loss: 10.5681 Explore P: 0.8295\n",
      "Episode: 40 Total reward: 94.0 Training loss: 23.4501 Explore P: 0.8289\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 92.0 Training loss: 11.6183 Explore P: 0.8282\n",
      "Episode: 42 Total reward: 36.0 Training loss: 7.6068 Explore P: 0.8237\n",
      "Episode: 43 Total reward: 93.0 Training loss: 4.4877 Explore P: 0.8231\n",
      "Episode: 45 Total reward: 95.0 Training loss: 4.5238 Explore P: 0.8145\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 95.0 Training loss: 23.0090 Explore P: 0.8140\n",
      "Episode: 47 Total reward: 32.0 Training loss: 12.0334 Explore P: 0.8097\n",
      "Episode: 48 Total reward: 95.0 Training loss: 9.1287 Explore P: 0.8092\n",
      "Episode: 49 Total reward: 95.0 Training loss: 14.3133 Explore P: 0.8087\n",
      "Episode: 50 Total reward: 49.0 Training loss: 9.4392 Explore P: 0.8054\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 93.0 Training loss: 8.5778 Explore P: 0.8047\n",
      "Episode: 52 Total reward: 93.0 Training loss: 10.8921 Explore P: 0.8041\n",
      "Episode: 53 Total reward: 95.0 Training loss: 5.7374 Explore P: 0.8036\n",
      "Episode: 54 Total reward: 94.0 Training loss: 17.9038 Explore P: 0.8031\n",
      "Episode: 55 Total reward: 93.0 Training loss: 7.3386 Explore P: 0.8024\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 94.0 Training loss: 18.5705 Explore P: 0.8019\n",
      "Episode: 57 Total reward: 95.0 Training loss: 29.4401 Explore P: 0.8014\n",
      "Episode: 58 Total reward: 91.0 Training loss: 10.3244 Explore P: 0.8006\n",
      "Episode: 59 Total reward: 79.0 Training loss: 13.0813 Explore P: 0.7989\n",
      "Episode: 60 Total reward: 94.0 Training loss: 10.4624 Explore P: 0.7983\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 19.0 Training loss: 9.8761 Explore P: 0.7931\n",
      "Episode: 62 Total reward: 95.0 Training loss: 7.3182 Explore P: 0.7926\n",
      "Episode: 63 Total reward: 89.0 Training loss: 5.3883 Explore P: 0.7916\n",
      "Episode: 64 Total reward: 95.0 Training loss: 13.9042 Explore P: 0.7912\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 94.0 Training loss: 6.6121 Explore P: 0.7829\n",
      "Episode: 67 Total reward: 63.0 Training loss: 14.4849 Explore P: 0.7803\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 94.0 Training loss: 6.9171 Explore P: 0.7570\n",
      "Episode: 72 Total reward: 95.0 Training loss: 24.9749 Explore P: 0.7566\n",
      "Episode: 73 Total reward: 93.0 Training loss: 10.5750 Explore P: 0.7560\n",
      "Episode: 74 Total reward: 93.0 Training loss: 10.2079 Explore P: 0.7554\n",
      "Model Saved\n",
      "Episode: 77 Total reward: 42.0 Training loss: 9.5849 Explore P: 0.7371\n",
      "Episode: 78 Total reward: 95.0 Training loss: 7.8622 Explore P: 0.7366\n",
      "Episode: 79 Total reward: 92.0 Training loss: 5.2849 Explore P: 0.7360\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -12.0 Training loss: 49.7061 Explore P: 0.7221\n",
      "Episode: 83 Total reward: 24.0 Training loss: 9.3585 Explore P: 0.7106\n",
      "Episode: 85 Total reward: 92.0 Training loss: 9.0666 Explore P: 0.7031\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 92.0 Training loss: 4.6390 Explore P: 0.7024\n",
      "Episode: 87 Total reward: 95.0 Training loss: 16.7819 Explore P: 0.7020\n",
      "Episode: 88 Total reward: 91.0 Training loss: 11.5755 Explore P: 0.7013\n",
      "Episode: 89 Total reward: 21.0 Training loss: 7.8676 Explore P: 0.6968\n",
      "Episode: 90 Total reward: 95.0 Training loss: 9.3229 Explore P: 0.6964\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 49.0 Training loss: 4.3853 Explore P: 0.6936\n",
      "Episode: 92 Total reward: 85.0 Training loss: 9.0853 Explore P: 0.6925\n",
      "Episode: 93 Total reward: 95.0 Training loss: 6.1602 Explore P: 0.6921\n",
      "Episode: 94 Total reward: 94.0 Training loss: 5.7875 Explore P: 0.6916\n",
      "Episode: 95 Total reward: 51.0 Training loss: 5.8790 Explore P: 0.6889\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -11.0 Training loss: 8.2996 Explore P: 0.6826\n",
      "Episode: 97 Total reward: 88.0 Training loss: 7.1965 Explore P: 0.6818\n",
      "Episode: 98 Total reward: 95.0 Training loss: 8.1815 Explore P: 0.6814\n",
      "Episode: 100 Total reward: 43.0 Training loss: 3.4409 Explore P: 0.6715\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 71.0 Training loss: 20.1970 Explore P: 0.6698\n",
      "Episode: 103 Total reward: 95.0 Training loss: 5.8467 Explore P: 0.6629\n",
      "Episode: 104 Total reward: 7.0 Training loss: 8.2192 Explore P: 0.6581\n",
      "Model Saved\n",
      "Episode: 107 Total reward: 94.0 Training loss: 7.3862 Explore P: 0.6448\n",
      "Episode: 108 Total reward: 91.0 Training loss: 5.8934 Explore P: 0.6442\n",
      "Episode: 110 Total reward: 91.0 Training loss: 7.0926 Explore P: 0.6372\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 76.0 Training loss: 22.3970 Explore P: 0.6360\n",
      "Episode: 112 Total reward: 95.0 Training loss: 11.3816 Explore P: 0.6356\n",
      "Episode: 113 Total reward: 95.0 Training loss: 8.4799 Explore P: 0.6352\n",
      "Episode: 115 Total reward: 95.0 Training loss: 9.3839 Explore P: 0.6286\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 69.0 Training loss: 6.4533 Explore P: 0.6270\n",
      "Episode: 117 Total reward: 93.0 Training loss: 12.4829 Explore P: 0.6265\n",
      "Episode: 118 Total reward: 95.0 Training loss: 9.7718 Explore P: 0.6261\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 64.0 Training loss: 5.0104 Explore P: 0.6120\n",
      "Episode: 122 Total reward: 93.0 Training loss: 7.8642 Explore P: 0.6115\n",
      "Episode: 123 Total reward: 93.0 Training loss: 7.4237 Explore P: 0.6110\n",
      "Episode: 124 Total reward: 76.0 Training loss: 4.8345 Explore P: 0.6098\n",
      "Episode: 125 Total reward: 95.0 Training loss: 118.7777 Explore P: 0.6094\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 18.0 Training loss: 8.8356 Explore P: 0.6054\n",
      "Episode: 127 Total reward: 69.0 Training loss: 15.6681 Explore P: 0.6038\n",
      "Episode: 128 Total reward: 95.0 Training loss: 8.6037 Explore P: 0.6034\n",
      "Episode: 129 Total reward: 95.0 Training loss: 14.4392 Explore P: 0.6031\n",
      "Episode: 130 Total reward: 93.0 Training loss: 10.2050 Explore P: 0.6026\n",
      "Model Saved\n",
      "Episode: 132 Total reward: 93.0 Training loss: 7.7207 Explore P: 0.5962\n",
      "Episode: 133 Total reward: 52.0 Training loss: 11.6264 Explore P: 0.5939\n",
      "Episode: 134 Total reward: 95.0 Training loss: 9.4299 Explore P: 0.5936\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 86.0 Training loss: 6.6233 Explore P: 0.5869\n",
      "Episode: 137 Total reward: 95.0 Training loss: 17.3976 Explore P: 0.5866\n",
      "Episode: 139 Total reward: 62.0 Training loss: 11.5746 Explore P: 0.5789\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 92.0 Training loss: 6.4232 Explore P: 0.5727\n",
      "Episode: 142 Total reward: 23.0 Training loss: 10.0160 Explore P: 0.5692\n",
      "Episode: 143 Total reward: 60.0 Training loss: 9.9622 Explore P: 0.5672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 144 Total reward: 95.0 Training loss: 32.3101 Explore P: 0.5669\n",
      "Episode: 145 Total reward: 5.0 Training loss: 9.3620 Explore P: 0.5624\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 95.0 Training loss: 6.1244 Explore P: 0.5620\n",
      "Episode: 147 Total reward: 58.0 Training loss: 10.0412 Explore P: 0.5599\n",
      "Episode: 148 Total reward: 22.0 Training loss: 11.0195 Explore P: 0.5564\n",
      "Episode: 149 Total reward: -9.0 Training loss: 8.9275 Explore P: 0.5515\n",
      "Episode: 150 Total reward: 93.0 Training loss: 6.5025 Explore P: 0.5511\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 57.0 Training loss: 6.6665 Explore P: 0.5493\n",
      "Episode: 152 Total reward: 18.0 Training loss: 7.2203 Explore P: 0.5456\n",
      "Episode: 153 Total reward: 47.0 Training loss: 22.2795 Explore P: 0.5433\n",
      "Episode: 154 Total reward: 36.0 Training loss: 4.1565 Explore P: 0.5403\n",
      "Episode: 155 Total reward: 76.0 Training loss: 13.6636 Explore P: 0.5393\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 95.0 Training loss: 11.7775 Explore P: 0.5390\n",
      "Episode: 157 Total reward: 94.0 Training loss: 6.9957 Explore P: 0.5386\n",
      "Episode: 158 Total reward: 94.0 Training loss: 14.1914 Explore P: 0.5382\n",
      "Episode: 159 Total reward: 95.0 Training loss: 6.5054 Explore P: 0.5379\n",
      "Episode: 160 Total reward: 94.0 Training loss: 6.2838 Explore P: 0.5375\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 94.0 Training loss: 5.3550 Explore P: 0.5372\n",
      "Episode: 162 Total reward: 87.0 Training loss: 7.1707 Explore P: 0.5364\n",
      "Episode: 164 Total reward: 70.0 Training loss: 13.1206 Explore P: 0.5298\n",
      "Episode: 165 Total reward: 90.0 Training loss: 11.3399 Explore P: 0.5293\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 92.0 Training loss: 8.7639 Explore P: 0.5288\n",
      "Episode: 167 Total reward: 95.0 Training loss: 5.8012 Explore P: 0.5285\n",
      "Episode: 169 Total reward: 44.0 Training loss: 8.7503 Explore P: 0.5209\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 66.0 Training loss: 9.2197 Explore P: 0.5143\n",
      "Episode: 172 Total reward: 94.0 Training loss: 8.0246 Explore P: 0.5140\n",
      "Episode: 174 Total reward: 63.0 Training loss: 8.2857 Explore P: 0.5073\n",
      "Episode: 175 Total reward: 95.0 Training loss: 5.9174 Explore P: 0.5070\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 64.0 Training loss: 7.1868 Explore P: 0.5054\n",
      "Episode: 177 Total reward: 66.0 Training loss: 29.2985 Explore P: 0.5039\n",
      "Episode: 178 Total reward: 95.0 Training loss: 6.1150 Explore P: 0.5036\n",
      "Episode: 179 Total reward: 87.0 Training loss: 7.7993 Explore P: 0.5029\n",
      "Episode: 180 Total reward: 94.0 Training loss: 5.9067 Explore P: 0.5026\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 95.0 Training loss: 11.3698 Explore P: 0.5023\n",
      "Episode: 183 Total reward: -4.0 Training loss: 40.0806 Explore P: 0.4933\n",
      "Episode: 184 Total reward: 95.0 Training loss: 7.1715 Explore P: 0.4930\n",
      "Episode: 185 Total reward: 95.0 Training loss: 14.2945 Explore P: 0.4927\n",
      "Model Saved\n",
      "Episode: 186 Total reward: 94.0 Training loss: 5.2774 Explore P: 0.4924\n",
      "Episode: 187 Total reward: 48.0 Training loss: 7.1266 Explore P: 0.4903\n",
      "Episode: 188 Total reward: 95.0 Training loss: 7.7055 Explore P: 0.4900\n",
      "Episode: 189 Total reward: 95.0 Training loss: 3.4393 Explore P: 0.4897\n",
      "Episode: 190 Total reward: 93.0 Training loss: 7.4032 Explore P: 0.4893\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 94.0 Training loss: 17.7656 Explore P: 0.4890\n",
      "Episode: 193 Total reward: 89.0 Training loss: 4.8151 Explore P: 0.4837\n",
      "Episode: 194 Total reward: 94.0 Training loss: 7.1371 Explore P: 0.4833\n",
      "Episode: 195 Total reward: 13.0 Training loss: 6.1351 Explore P: 0.4799\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 95.0 Training loss: 3.5366 Explore P: 0.4796\n",
      "Episode: 197 Total reward: 68.0 Training loss: 6.8283 Explore P: 0.4783\n",
      "Episode: 198 Total reward: 42.0 Training loss: 4.3217 Explore P: 0.4760\n",
      "Episode: 199 Total reward: 95.0 Training loss: 5.2611 Explore P: 0.4757\n",
      "Episode: 200 Total reward: 88.0 Training loss: 6.8792 Explore P: 0.4751\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 95.0 Training loss: 4.9818 Explore P: 0.4748\n",
      "Episode: 202 Total reward: 24.0 Training loss: 7.1488 Explore P: 0.4720\n",
      "Episode: 203 Total reward: 95.0 Training loss: 13.5095 Explore P: 0.4717\n",
      "Episode: 204 Total reward: 67.0 Training loss: 8.7834 Explore P: 0.4704\n",
      "Episode: 205 Total reward: 95.0 Training loss: 5.0097 Explore P: 0.4701\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 34.0 Training loss: 8.3197 Explore P: 0.4675\n",
      "Episode: 207 Total reward: 93.0 Training loss: 3.9446 Explore P: 0.4671\n",
      "Episode: 208 Total reward: 50.0 Training loss: 9.9673 Explore P: 0.4652\n",
      "Episode: 209 Total reward: 28.0 Training loss: 6.9026 Explore P: 0.4626\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 95.0 Training loss: 6.4600 Explore P: 0.4578\n",
      "Episode: 212 Total reward: 23.0 Training loss: 6.5460 Explore P: 0.4548\n",
      "Episode: 213 Total reward: 70.0 Training loss: 11.3913 Explore P: 0.4536\n",
      "Episode: 214 Total reward: 95.0 Training loss: 8.3840 Explore P: 0.4534\n",
      "Episode: 215 Total reward: 95.0 Training loss: 8.7881 Explore P: 0.4531\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 91.0 Training loss: 7.5967 Explore P: 0.4527\n",
      "Episode: 217 Total reward: 95.0 Training loss: 4.8283 Explore P: 0.4524\n",
      "Episode: 218 Total reward: 95.0 Training loss: 5.1091 Explore P: 0.4521\n",
      "Episode: 219 Total reward: 68.0 Training loss: 5.0548 Explore P: 0.4509\n",
      "Episode: 220 Total reward: 95.0 Training loss: 4.3652 Explore P: 0.4506\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 76.0 Training loss: 9.9714 Explore P: 0.4497\n",
      "Episode: 222 Total reward: 94.0 Training loss: 7.9252 Explore P: 0.4494\n",
      "Episode: 223 Total reward: 95.0 Training loss: 5.4676 Explore P: 0.4492\n",
      "Episode: 224 Total reward: 95.0 Training loss: 3.9259 Explore P: 0.4489\n",
      "Episode: 225 Total reward: 35.0 Training loss: 14.3621 Explore P: 0.4465\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 94.0 Training loss: 5.7061 Explore P: 0.4462\n",
      "Episode: 227 Total reward: 20.0 Training loss: 4.1670 Explore P: 0.4433\n",
      "Episode: 228 Total reward: 95.0 Training loss: 6.3422 Explore P: 0.4430\n",
      "Episode: 229 Total reward: 95.0 Training loss: 11.7960 Explore P: 0.4428\n",
      "Episode: 230 Total reward: 87.0 Training loss: 3.6307 Explore P: 0.4422\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 93.0 Training loss: 10.1309 Explore P: 0.4418\n",
      "Episode: 232 Total reward: 49.0 Training loss: 19.3779 Explore P: 0.4400\n",
      "Episode: 233 Total reward: 49.0 Training loss: 8.4461 Explore P: 0.4382\n",
      "Episode: 234 Total reward: 95.0 Training loss: 6.8993 Explore P: 0.4379\n",
      "Model Saved\n",
      "Episode: 237 Total reward: 33.0 Training loss: 5.9109 Explore P: 0.4270\n",
      "Episode: 238 Total reward: 74.0 Training loss: 4.6369 Explore P: 0.4261\n",
      "Episode: 239 Total reward: 47.0 Training loss: 9.3853 Explore P: 0.4243\n",
      "Episode: 240 Total reward: 95.0 Training loss: 4.4259 Explore P: 0.4241\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 66.0 Training loss: 5.1215 Explore P: 0.4228\n",
      "Episode: 242 Total reward: 95.0 Training loss: 7.5632 Explore P: 0.4226\n",
      "Episode: 243 Total reward: 95.0 Training loss: 7.0886 Explore P: 0.4223\n",
      "Episode: 244 Total reward: 95.0 Training loss: 3.1156 Explore P: 0.4221\n",
      "Episode: 245 Total reward: 95.0 Training loss: 4.9253 Explore P: 0.4218\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 95.0 Training loss: 6.1414 Explore P: 0.4216\n",
      "Episode: 247 Total reward: 95.0 Training loss: 10.0147 Explore P: 0.4213\n",
      "Episode: 248 Total reward: 95.0 Training loss: 7.7575 Explore P: 0.4211\n",
      "Episode: 249 Total reward: 94.0 Training loss: 6.3641 Explore P: 0.4208\n",
      "Episode: 250 Total reward: 95.0 Training loss: 5.8558 Explore P: 0.4206\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 13.0 Training loss: 7.2306 Explore P: 0.4176\n",
      "Episode: 252 Total reward: 95.0 Training loss: 9.2144 Explore P: 0.4173\n",
      "Episode: 253 Total reward: 75.0 Training loss: 3.6794 Explore P: 0.4165\n",
      "Episode: 254 Total reward: 95.0 Training loss: 4.5388 Explore P: 0.4162\n",
      "Episode: 255 Total reward: 45.0 Training loss: 7.5836 Explore P: 0.4144\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 47.0 Training loss: 3.8475 Explore P: 0.4124\n",
      "Episode: 257 Total reward: 92.0 Training loss: 5.1477 Explore P: 0.4120\n",
      "Episode: 258 Total reward: 26.0 Training loss: 5.2592 Explore P: 0.4096\n",
      "Episode: 259 Total reward: 71.0 Training loss: 7.9127 Explore P: 0.4086\n",
      "Episode: 260 Total reward: 95.0 Training loss: 9.7758 Explore P: 0.4084\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 44.0 Training loss: 5.2325 Explore P: 0.4065\n",
      "Episode: 262 Total reward: 94.0 Training loss: 4.2870 Explore P: 0.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 263 Total reward: 95.0 Training loss: 8.1604 Explore P: 0.4060\n",
      "Episode: 264 Total reward: 69.0 Training loss: 13.4942 Explore P: 0.4049\n",
      "Episode: 265 Total reward: 43.0 Training loss: 9.9902 Explore P: 0.4030\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 43.0 Training loss: 4.1344 Explore P: 0.4012\n",
      "Episode: 267 Total reward: 95.0 Training loss: 6.8788 Explore P: 0.4009\n",
      "Episode: 268 Total reward: 69.0 Training loss: 12.3378 Explore P: 0.3999\n",
      "Episode: 269 Total reward: 95.0 Training loss: 7.9525 Explore P: 0.3996\n",
      "Model Saved\n",
      "Episode: 272 Total reward: 95.0 Training loss: 4.3477 Explore P: 0.3917\n",
      "Episode: 273 Total reward: 75.0 Training loss: 6.9403 Explore P: 0.3909\n",
      "Episode: 274 Total reward: 95.0 Training loss: 6.1009 Explore P: 0.3907\n",
      "Episode: 275 Total reward: 95.0 Training loss: 6.6388 Explore P: 0.3904\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 95.0 Training loss: 2.6782 Explore P: 0.3902\n",
      "Episode: 277 Total reward: 95.0 Training loss: 11.7640 Explore P: 0.3900\n",
      "Episode: 278 Total reward: 95.0 Training loss: 4.6177 Explore P: 0.3897\n",
      "Episode: 279 Total reward: 95.0 Training loss: 7.2396 Explore P: 0.3895\n",
      "Episode: 280 Total reward: 47.0 Training loss: 4.4909 Explore P: 0.3879\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 95.0 Training loss: 6.8512 Explore P: 0.3876\n",
      "Episode: 282 Total reward: 94.0 Training loss: 4.3220 Explore P: 0.3874\n",
      "Episode: 283 Total reward: 95.0 Training loss: 6.5364 Explore P: 0.3871\n",
      "Episode: 284 Total reward: 68.0 Training loss: 3.8518 Explore P: 0.3861\n",
      "Episode: 285 Total reward: 93.0 Training loss: 5.3309 Explore P: 0.3858\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 68.0 Training loss: 4.2554 Explore P: 0.3847\n",
      "Episode: 287 Total reward: 95.0 Training loss: 6.4478 Explore P: 0.3845\n",
      "Episode: 288 Total reward: 95.0 Training loss: 4.6351 Explore P: 0.3843\n",
      "Episode: 289 Total reward: 95.0 Training loss: 6.4310 Explore P: 0.3841\n",
      "Episode: 290 Total reward: 95.0 Training loss: 4.5709 Explore P: 0.3838\n",
      "Model Saved\n",
      "Episode: 292 Total reward: 65.0 Training loss: 3.9258 Explore P: 0.3790\n",
      "Episode: 293 Total reward: 95.0 Training loss: 5.9661 Explore P: 0.3787\n",
      "Episode: 294 Total reward: 95.0 Training loss: 6.5785 Explore P: 0.3785\n",
      "Episode: 295 Total reward: 66.0 Training loss: 9.4675 Explore P: 0.3774\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 95.0 Training loss: 13.2869 Explore P: 0.3772\n",
      "Episode: 297 Total reward: 94.0 Training loss: 7.5453 Explore P: 0.3769\n",
      "Episode: 298 Total reward: 76.0 Training loss: 17.3079 Explore P: 0.3762\n",
      "Episode: 299 Total reward: 95.0 Training loss: 5.8582 Explore P: 0.3760\n",
      "Episode: 300 Total reward: 93.0 Training loss: 3.3119 Explore P: 0.3757\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 95.0 Training loss: 6.7532 Explore P: 0.3755\n",
      "Episode: 302 Total reward: 95.0 Training loss: 5.9741 Explore P: 0.3753\n",
      "Episode: 303 Total reward: 95.0 Training loss: 3.7473 Explore P: 0.3750\n",
      "Episode: 304 Total reward: 63.0 Training loss: 8.1089 Explore P: 0.3738\n",
      "Episode: 305 Total reward: 95.0 Training loss: 9.6559 Explore P: 0.3736\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 95.0 Training loss: 3.8454 Explore P: 0.3734\n",
      "Episode: 307 Total reward: 95.0 Training loss: 9.4842 Explore P: 0.3732\n",
      "Episode: 308 Total reward: 95.0 Training loss: 6.7452 Explore P: 0.3730\n",
      "Episode: 309 Total reward: 75.0 Training loss: 8.6794 Explore P: 0.3722\n",
      "Episode: 310 Total reward: 94.0 Training loss: 3.4921 Explore P: 0.3719\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 95.0 Training loss: 7.3742 Explore P: 0.3717\n",
      "Episode: 312 Total reward: 22.0 Training loss: 23.2003 Explore P: 0.3694\n",
      "Episode: 313 Total reward: 95.0 Training loss: 10.1249 Explore P: 0.3692\n",
      "Episode: 314 Total reward: 45.0 Training loss: 5.9439 Explore P: 0.3676\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 88.0 Training loss: 6.0934 Explore P: 0.3635\n",
      "Episode: 317 Total reward: 94.0 Training loss: 5.6897 Explore P: 0.3633\n",
      "Episode: 319 Total reward: 92.0 Training loss: 3.3719 Explore P: 0.3595\n",
      "Episode: 320 Total reward: 95.0 Training loss: 6.1863 Explore P: 0.3593\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 94.0 Training loss: 12.2062 Explore P: 0.3590\n",
      "Episode: 322 Total reward: 93.0 Training loss: 8.1730 Explore P: 0.3587\n",
      "Episode: 325 Total reward: 94.0 Training loss: 14.3451 Explore P: 0.3516\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 95.0 Training loss: 15.0316 Explore P: 0.3514\n",
      "Episode: 327 Total reward: 75.0 Training loss: 4.9020 Explore P: 0.3507\n",
      "Episode: 328 Total reward: 76.0 Training loss: 5.4696 Explore P: 0.3500\n",
      "Episode: 329 Total reward: 95.0 Training loss: 8.5457 Explore P: 0.3498\n",
      "Episode: 330 Total reward: 68.0 Training loss: 4.9648 Explore P: 0.3488\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 95.0 Training loss: 3.6702 Explore P: 0.3486\n",
      "Episode: 334 Total reward: 94.0 Training loss: 10.7239 Explore P: 0.3417\n",
      "Episode: 335 Total reward: 95.0 Training loss: 5.7909 Explore P: 0.3415\n",
      "Model Saved\n",
      "Episode: 337 Total reward: 94.0 Training loss: 10.5206 Explore P: 0.3380\n",
      "Episode: 339 Total reward: 75.0 Training loss: 6.0277 Explore P: 0.3340\n",
      "Episode: 340 Total reward: 40.0 Training loss: 3.0969 Explore P: 0.3324\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 95.0 Training loss: 4.8824 Explore P: 0.3322\n",
      "Episode: 342 Total reward: 95.0 Training loss: 5.4162 Explore P: 0.3320\n",
      "Episode: 343 Total reward: 94.0 Training loss: 4.5656 Explore P: 0.3318\n",
      "Episode: 345 Total reward: 94.0 Training loss: 6.8480 Explore P: 0.3283\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 91.0 Training loss: 8.6073 Explore P: 0.3280\n",
      "Episode: 347 Total reward: 52.0 Training loss: 2.7886 Explore P: 0.3266\n",
      "Episode: 348 Total reward: 95.0 Training loss: 5.3888 Explore P: 0.3264\n",
      "Episode: 349 Total reward: 95.0 Training loss: 24.9111 Explore P: 0.3262\n",
      "Episode: 350 Total reward: 40.0 Training loss: 4.1769 Explore P: 0.3246\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 95.0 Training loss: 5.2864 Explore P: 0.3244\n",
      "Episode: 352 Total reward: 95.0 Training loss: 5.8215 Explore P: 0.3243\n",
      "Episode: 353 Total reward: 94.0 Training loss: 4.3464 Explore P: 0.3240\n",
      "Episode: 354 Total reward: 94.0 Training loss: 4.9739 Explore P: 0.3238\n",
      "Episode: 355 Total reward: 82.0 Training loss: 4.7443 Explore P: 0.3232\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 55.0 Training loss: 5.4619 Explore P: 0.3221\n",
      "Episode: 357 Total reward: 89.0 Training loss: 5.1024 Explore P: 0.3217\n",
      "Episode: 358 Total reward: 95.0 Training loss: 4.2882 Explore P: 0.3215\n",
      "Episode: 359 Total reward: 94.0 Training loss: 5.1731 Explore P: 0.3213\n",
      "Episode: 360 Total reward: 26.0 Training loss: 5.9593 Explore P: 0.3195\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 64.0 Training loss: 4.9812 Explore P: 0.3185\n",
      "Episode: 362 Total reward: 95.0 Training loss: 4.0822 Explore P: 0.3183\n",
      "Episode: 363 Total reward: 67.0 Training loss: 5.9500 Explore P: 0.3174\n",
      "Episode: 364 Total reward: 56.0 Training loss: 3.9405 Explore P: 0.3163\n",
      "Episode: 365 Total reward: 13.0 Training loss: 4.6155 Explore P: 0.3141\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 95.0 Training loss: 2.9210 Explore P: 0.3139\n",
      "Episode: 367 Total reward: 95.0 Training loss: 4.9687 Explore P: 0.3137\n",
      "Episode: 368 Total reward: 95.0 Training loss: 5.4024 Explore P: 0.3135\n",
      "Episode: 369 Total reward: 62.0 Training loss: 5.4297 Explore P: 0.3125\n",
      "Episode: 370 Total reward: 38.0 Training loss: 5.6882 Explore P: 0.3109\n",
      "Model Saved\n",
      "Episode: 372 Total reward: 95.0 Training loss: 15.2884 Explore P: 0.3077\n",
      "Episode: 373 Total reward: 95.0 Training loss: 3.6671 Explore P: 0.3076\n",
      "Episode: 374 Total reward: 95.0 Training loss: 9.6644 Explore P: 0.3074\n",
      "Episode: 375 Total reward: 44.0 Training loss: 6.3367 Explore P: 0.3060\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 95.0 Training loss: 2.8846 Explore P: 0.3058\n",
      "Episode: 377 Total reward: 95.0 Training loss: 5.3152 Explore P: 0.3056\n",
      "Episode: 378 Total reward: 91.0 Training loss: 3.5286 Explore P: 0.3053\n",
      "Episode: 379 Total reward: 91.0 Training loss: 6.1341 Explore P: 0.3050\n",
      "Episode: 380 Total reward: 29.0 Training loss: 3.3842 Explore P: 0.3034\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 95.0 Training loss: 7.9459 Explore P: 0.3032\n",
      "Episode: 382 Total reward: 63.0 Training loss: 10.9119 Explore P: 0.3022\n",
      "Episode: 383 Total reward: 95.0 Training loss: 6.0767 Explore P: 0.3020\n",
      "Episode: 384 Total reward: 95.0 Training loss: 3.7858 Explore P: 0.3019\n",
      "Episode: 385 Total reward: 95.0 Training loss: 9.0379 Explore P: 0.3017\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 386 Total reward: 94.0 Training loss: 2.5688 Explore P: 0.3015\n",
      "Episode: 388 Total reward: 94.0 Training loss: 7.0150 Explore P: 0.2984\n",
      "Episode: 389 Total reward: 50.0 Training loss: 6.7868 Explore P: 0.2972\n",
      "Episode: 390 Total reward: 50.0 Training loss: 4.0695 Explore P: 0.2960\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 95.0 Training loss: 2.5931 Explore P: 0.2959\n",
      "Episode: 392 Total reward: 47.0 Training loss: 5.6936 Explore P: 0.2946\n",
      "Episode: 393 Total reward: 94.0 Training loss: 5.1065 Explore P: 0.2944\n",
      "Episode: 394 Total reward: 95.0 Training loss: 4.7621 Explore P: 0.2942\n",
      "Episode: 395 Total reward: 95.0 Training loss: 5.3101 Explore P: 0.2941\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 94.0 Training loss: 5.2702 Explore P: 0.2939\n",
      "Episode: 397 Total reward: 71.0 Training loss: 2.4092 Explore P: 0.2932\n",
      "Episode: 398 Total reward: 43.0 Training loss: 9.7507 Explore P: 0.2917\n",
      "Episode: 399 Total reward: 95.0 Training loss: 22.6189 Explore P: 0.2915\n",
      "Episode: 400 Total reward: 70.0 Training loss: 3.6550 Explore P: 0.2908\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 95.0 Training loss: 17.7400 Explore P: 0.2906\n",
      "Episode: 402 Total reward: 20.0 Training loss: 5.6874 Explore P: 0.2887\n",
      "Episode: 403 Total reward: 95.0 Training loss: 5.0350 Explore P: 0.2886\n",
      "Episode: 404 Total reward: 35.0 Training loss: 4.6416 Explore P: 0.2872\n",
      "Episode: 405 Total reward: 92.0 Training loss: 6.6260 Explore P: 0.2869\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 66.0 Training loss: 6.0675 Explore P: 0.2861\n",
      "Episode: 407 Total reward: 71.0 Training loss: 17.0018 Explore P: 0.2854\n",
      "Episode: 408 Total reward: 93.0 Training loss: 7.2653 Explore P: 0.2852\n",
      "Episode: 409 Total reward: 87.0 Training loss: 6.7291 Explore P: 0.2848\n",
      "Episode: 410 Total reward: 95.0 Training loss: 2.6874 Explore P: 0.2846\n",
      "Model Saved\n",
      "Episode: 412 Total reward: 94.0 Training loss: 24.6430 Explore P: 0.2817\n",
      "Episode: 413 Total reward: 95.0 Training loss: 32.4571 Explore P: 0.2815\n",
      "Episode: 414 Total reward: 95.0 Training loss: 9.7234 Explore P: 0.2814\n",
      "Episode: 415 Total reward: 62.0 Training loss: 5.6922 Explore P: 0.2805\n",
      "Model Saved\n",
      "Episode: 417 Total reward: 63.0 Training loss: 7.4504 Explore P: 0.2769\n",
      "Episode: 418 Total reward: 95.0 Training loss: 29.5133 Explore P: 0.2767\n",
      "Episode: 419 Total reward: 57.0 Training loss: 17.5531 Explore P: 0.2757\n",
      "Episode: 420 Total reward: 95.0 Training loss: 5.3615 Explore P: 0.2755\n",
      "Model Saved\n",
      "Episode: 422 Total reward: 93.0 Training loss: 5.8139 Explore P: 0.2727\n",
      "Episode: 423 Total reward: 88.0 Training loss: 9.7028 Explore P: 0.2723\n",
      "Episode: 424 Total reward: 95.0 Training loss: 10.9109 Explore P: 0.2722\n",
      "Episode: 425 Total reward: 95.0 Training loss: 6.1222 Explore P: 0.2720\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 30.0 Training loss: 8.2472 Explore P: 0.2704\n",
      "Episode: 427 Total reward: 26.0 Training loss: 2.7541 Explore P: 0.2689\n",
      "Episode: 428 Total reward: 95.0 Training loss: 7.9370 Explore P: 0.2687\n",
      "Episode: 429 Total reward: 95.0 Training loss: 7.0117 Explore P: 0.2686\n",
      "Episode: 430 Total reward: 95.0 Training loss: 8.0016 Explore P: 0.2684\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 95.0 Training loss: 4.0432 Explore P: 0.2682\n",
      "Episode: 432 Total reward: 67.0 Training loss: 5.8932 Explore P: 0.2675\n",
      "Episode: 433 Total reward: 95.0 Training loss: 10.5249 Explore P: 0.2673\n",
      "Episode: 434 Total reward: 95.0 Training loss: 4.4308 Explore P: 0.2672\n",
      "Episode: 435 Total reward: 95.0 Training loss: 1.8643 Explore P: 0.2670\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 95.0 Training loss: 3.2588 Explore P: 0.2669\n",
      "Episode: 437 Total reward: 95.0 Training loss: 5.7244 Explore P: 0.2667\n",
      "Episode: 438 Total reward: 94.0 Training loss: 7.1172 Explore P: 0.2665\n",
      "Episode: 439 Total reward: 66.0 Training loss: 28.0315 Explore P: 0.2658\n",
      "Episode: 440 Total reward: 95.0 Training loss: 3.3320 Explore P: 0.2656\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 66.0 Training loss: 4.2692 Explore P: 0.2649\n",
      "Episode: 442 Total reward: 65.0 Training loss: 6.7897 Explore P: 0.2641\n",
      "Episode: 443 Total reward: 95.0 Training loss: 30.7177 Explore P: 0.2639\n",
      "Episode: 444 Total reward: 95.0 Training loss: 3.8250 Explore P: 0.2638\n",
      "Episode: 445 Total reward: 65.0 Training loss: 4.3885 Explore P: 0.2630\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 64.0 Training loss: 6.5072 Explore P: 0.2622\n",
      "Episode: 447 Total reward: 95.0 Training loss: 8.9319 Explore P: 0.2620\n",
      "Episode: 448 Total reward: 38.0 Training loss: 5.6911 Explore P: 0.2607\n",
      "Episode: 449 Total reward: 95.0 Training loss: 3.5733 Explore P: 0.2605\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 66.0 Training loss: 3.1533 Explore P: 0.2573\n",
      "Episode: 452 Total reward: 76.0 Training loss: 5.0172 Explore P: 0.2568\n",
      "Episode: 453 Total reward: 95.0 Training loss: 3.0285 Explore P: 0.2567\n",
      "Episode: 454 Total reward: 95.0 Training loss: 5.9627 Explore P: 0.2565\n",
      "Episode: 455 Total reward: 95.0 Training loss: 5.5964 Explore P: 0.2564\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 88.0 Training loss: 6.4033 Explore P: 0.2560\n",
      "Episode: 457 Total reward: 40.0 Training loss: 10.7533 Explore P: 0.2548\n",
      "Episode: 458 Total reward: 95.0 Training loss: 1.9145 Explore P: 0.2546\n",
      "Episode: 459 Total reward: 95.0 Training loss: 3.4220 Explore P: 0.2545\n",
      "Episode: 460 Total reward: 58.0 Training loss: 5.4723 Explore P: 0.2536\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 90.0 Training loss: 14.8583 Explore P: 0.2533\n",
      "Episode: 462 Total reward: 95.0 Training loss: 2.7902 Explore P: 0.2532\n",
      "Episode: 463 Total reward: 95.0 Training loss: 20.6878 Explore P: 0.2530\n",
      "Episode: 464 Total reward: 95.0 Training loss: 8.3818 Explore P: 0.2529\n",
      "Episode: 465 Total reward: 94.0 Training loss: 3.5136 Explore P: 0.2527\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 95.0 Training loss: 5.3306 Explore P: 0.2525\n",
      "Episode: 467 Total reward: 95.0 Training loss: 3.9644 Explore P: 0.2524\n",
      "Episode: 468 Total reward: 95.0 Training loss: 3.1186 Explore P: 0.2523\n",
      "Episode: 469 Total reward: 67.0 Training loss: 7.3562 Explore P: 0.2516\n",
      "Episode: 470 Total reward: 95.0 Training loss: 3.9004 Explore P: 0.2514\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 94.0 Training loss: 68.7149 Explore P: 0.2512\n",
      "Episode: 472 Total reward: 75.0 Training loss: 7.7966 Explore P: 0.2507\n",
      "Episode: 473 Total reward: 95.0 Training loss: 2.1756 Explore P: 0.2506\n",
      "Episode: 474 Total reward: 60.0 Training loss: 5.3364 Explore P: 0.2497\n",
      "Episode: 475 Total reward: 52.0 Training loss: 5.5095 Explore P: 0.2487\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 95.0 Training loss: 5.6511 Explore P: 0.2485\n",
      "Episode: 477 Total reward: 66.0 Training loss: 3.8725 Explore P: 0.2478\n",
      "Episode: 478 Total reward: 67.0 Training loss: 4.3253 Explore P: 0.2471\n",
      "Episode: 479 Total reward: 95.0 Training loss: 4.4795 Explore P: 0.2470\n",
      "Episode: 480 Total reward: 91.0 Training loss: 7.0158 Explore P: 0.2468\n",
      "Model Saved\n",
      "Episode: 482 Total reward: 51.0 Training loss: 4.5957 Explore P: 0.2435\n",
      "Episode: 483 Total reward: 95.0 Training loss: 5.0850 Explore P: 0.2433\n",
      "Episode: 484 Total reward: 57.0 Training loss: 4.7543 Explore P: 0.2424\n",
      "Episode: 485 Total reward: 95.0 Training loss: 7.9942 Explore P: 0.2423\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 82.0 Training loss: 9.4289 Explore P: 0.2418\n",
      "Episode: 487 Total reward: 94.0 Training loss: 2.7186 Explore P: 0.2417\n",
      "Episode: 488 Total reward: 35.0 Training loss: 4.4778 Explore P: 0.2404\n",
      "Episode: 489 Total reward: 69.0 Training loss: 4.1951 Explore P: 0.2398\n",
      "Episode: 490 Total reward: 94.0 Training loss: 6.0448 Explore P: 0.2396\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 95.0 Training loss: 5.7061 Explore P: 0.2395\n",
      "Episode: 492 Total reward: 95.0 Training loss: 7.4450 Explore P: 0.2393\n",
      "Episode: 493 Total reward: 94.0 Training loss: 12.2515 Explore P: 0.2392\n",
      "Episode: 494 Total reward: 95.0 Training loss: 7.8596 Explore P: 0.2390\n",
      "Episode: 495 Total reward: 75.0 Training loss: 6.1085 Explore P: 0.2385\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 50.0 Training loss: 3.4729 Explore P: 0.2375\n",
      "Episode: 498 Total reward: 50.0 Training loss: 8.9641 Explore P: 0.2343\n",
      "Episode: 499 Total reward: 95.0 Training loss: 3.8911 Explore P: 0.2342\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state = preprocess_frame(state)\n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state = preprocess_frame(next_state)\n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "(3, 240, 4, 320)\n",
      "(3, 240, 4, 320)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'DQNetwork/inputs:0', which has shape '(?, 84, 84, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-60983fc6dd7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# Take the biggest Q value (= the best action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m#             Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\romul\\documents\\code\\drl-freecodecamp\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\romul\\documents\\code\\drl-freecodecamp\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1157\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'DQNetwork/inputs:0', which has shape '(?, 84, 84, 4)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "#     saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        print(state.shape)\n",
    "#         state = state.reshape(4,240,320,3)  \n",
    "#         state.resize(1,84,84,4)\n",
    "#         print(state.shape)\n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            print(state.shape)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.resize(1,84,84,4)})\n",
    "#             Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
